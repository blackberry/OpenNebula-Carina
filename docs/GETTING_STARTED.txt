
The server components for opennebula-carina are installed in a management
VM that can be accessed from the OpenNebula front-end where onecmd is
used to access OpenNebula commands.

1. Install rest-client ruby gem locally in your service account on the oZones
   or ONE server where you normally login to run ONE commands. If the admin
   has already installed it at the system level, this step is not necessary:

    gem install --install-dir ~/gems  /tmp/rest-client-1.6.7.gem

   There is an error in ~/gems/specifications/mime-types-1.18.gemspec
   that gets installed: Change
    s.date = %q{2012-03-21 00:00:00.000000000Z}
   to:
    s.date = %q{2012-03-21}


2. Copy the 'oneenv' CLI in your local bin directory

3. Set the following environment variables on the service account
on ring00
    export GEM_HOME=/var/lib/gems/1.8
    export GEM_PATH=/var/lib/gems/1.8:/home/<svc>/gems
    export CARINA_IP=5.62.27.190
    export CARINA_PORT=4567  (this is service-specific port that points to the oneenvd)
    export CARINA_GS_PORT=4321



4. Create the config.rb containing the endpoint, template, and at least
one environment definition.  The following is a minimal example:

ENDPOINT = {
        'flm01' => {
                :proxy   => 'http://ring00a/flm01',
                :oneauth => 'flm01-oneenv2:XXXXXX'
                }
            }



TEMPLATE = {
         'example' => {
                :file       => '/home/oneenv2/vm/example.vm',
                :cpu        => 1,
                :memory     => 1024,
                :network_id => { 'flm01' => 21, 'hlx01' => 7},
                :image_id   => { 'flm01' => 63, 'hlx01' => 20 }
        }
}

ENVIRONMENT = {
         'tomcat' => {
                :type => "compute",
                :endpoint => "flm01",
                :description => "Load-balanced Tomcat cluster",
                :placement_policy  => "pack",
                :master_template   => "example",
                :master_context_script =>  "balance.sh",
                :master_setup_time => 30,
                :master_context_var => "BALANCE_PORT=8080",
                :slave_template    => "example",
                :slave_context_script => "tomcat.sh",
                :slave_context_var => "\"APP_PACKAGE=gwt-petstore.war, SLAVE_ID=%SLAVE_INDEX%\"",
                :num_slaves        => 2,
                :slavedata         => "8080",
                :adminuser         => "local",
                :app_url           => "http://%MASTER%:8080/gwt-petstore"
         }
}


The following explains the parameters:

-type: a string allowing to classify different types of environments

-endpoint: specifies the endpoint defined in the ENDPOINT section that
would be used to create the environment.

-description: string describing the environment

-placement_policy: specifies how VMs for the environment should be
placed on physical hypervisors. Allowable values are 'pack', 'stripe'
or 'load'. This is used to select the RANK value in OpenNebula. See
http://www.opennebula.org/documentation:archives:rel3.0:schg.

-master_template, slave_template: references the TEMPLATEs section to
define what template will be used in creating the master and slave VMs.

-master_setup_time: delay between creating the master VM and the slave
VMs in order to give time for master setup to complete

-master_context_var, slave_context_var: Context variables to be
passed to the VM contextualization scripts on the master and slave
respectively. For the slave context vars, there are two strings that
are dynamically substituted at run time: %MASTER% is substituted with
the IP address of the master and %SLAVE_INDEX% is substituted with the
index number assigned to the slave starting at 0,1,2...

-num_slaves: the initial number of slaves that will be started when the
environment is first created

-slavedata: this is a parameter that is passed to slave

-adminuser: this is the user that exists in the slave VM that is used
as the account to ssh into to run commands

-app_url: this is used to define an app-specifc url that a user can
connect to see after the app has been successfully provisioned. The string
%MASTER% can be specified to resolve to the IP address of the master VM.

5. Upload the config.rb to the oneenvd server

     oneenv --upload config.rb

6. Update any VM templates to include the following context variables
and upload the VM file

CONTEXT        = [
  ENVID=%ENVID%,
  VMID=$VMID,
  CARINA_IP=%CARINA_IP%,
  SERVICE_NAME=%SERVICE_NAME%,
  DEFUSER=local,
  %APP_CONTEXT_VAR%,
  FILES        = "http://%CARINA_IP%/repo/%SERVICE_NAME%/context/authorized_keys http://ring00/repo/%SERVICE_NAME%/context/init.sh http://ring00/repo/%SERVICE_NAME%/context/network.sh  http://ring00/repo/%SERVICE_NAME%/context/%APP_CONTEXT_SCRIPT%",
.
.
]


These variables are resolved by the system. The authorized_keys file is
already generated in the controller VM and is used to ssh into master or
load-balancer VM without requiring password.  A service needs to provide
the contextualization scripts that will be invoked. A number of samples
are provided in the context subdirectory.

To upload the VM file do:
     oneenv --upload example.vm

Where example.vm is found in ~/vm/example.vm (i.e just put name not
full path)


7. Adapting contextualization scripts

The contextualization scripts have to be adapted to report status of
deployment back to the onenvd controller. The status is reported by
invoking a cgi script running on the controller passing it information
that will update the database. The parameters for the call such as ENVID,
VMID, etc are passed to the contextualization script automatically. The
following are examples of reporting status from within a contextualization
script.


# This should be invoked after the network contextualization is completed.
wget $CARINA_IP/cgi-bin/updatevmstatus.sh?service=$SERVICE_NAME\&vmid=$VMID\&envid=$ENVID\&newip=$IP\&name=$HOSTNAME 2> /dev/null

# This is invoked on the master VM for an environment to indicate successful setup.
wget $CARINA_IP/cgi-bin/updateappstatus.sh?service=$SERVICE_NAME\&vmid=$VMID\&envid=$ENVID\&status=MASTER_INIT_DONE 2> /dev/null


# This is invoked on the master VM for an environment to indicate a failed setup
wget $CARINA_IP/cgi-bin/updateappstatus.sh?service=$SERVICE_NAME\&vmid=$VMID\&envid=$ENVID\&status=MASTER_INIT_FAIL 2> /dev/null

NOTE: The system looks for the status of MASTER_INIT_DONE or MASTER_INIT_FAIL to start deploying the slaves.

# This is invoked on the slave VM for an environment to indicate successful setup
wget $CARINA_IP/cgi-bin/updateappstatus.sh?service=$SERVICE_NAME\&vmid=$VMID\&envid=$ENVID\&status=SLAVE_"$SLAVE_ID"_INIT_DONE 2> /dev/null


# This is invoked on the slave VM for an environment to indicate failed setup
wget $CARINA_IP/cgi-bin/updateappstatus.sh?service=$SERVICE_NAME\&vmid=$VMID\&envid=$ENVID\&status=SLAVE_"$SLAVE_ID"_INIT_FAIL 2> /dev/null

The master contextualization script is invoked when the VM is
created, but also when the a new slave is added or removed from the
environment. This is done by ssh-ing into the master VM and running the
master contextualization script as follows:

    # To inform the master to add a new slave
     ssh -o StrictHostKeyChecking=no $ADMINUSER@$MASTERIP /home/$ADMINUSER/$MASTER_CONTEXT_SCRIPT add $SLAVEIP $SLAVEDATA

     # To inform the master to remove a slave
     ssh -o StrictHostKeyChecking=no $ADMINUSER@$MASTERIP /home/$ADMINUSER/$MASTER_CONTEXT_SCRIPT delete $LASTSLAVEIP $SLAVEDATA


The master contextualization script should be written to accept the 'add'
and'remove' arguements. See the 'balance.sh' for an example of how this
is used to implement load-balancer integration.


8. Elasticity Policy Specification

The environment definition in the config.rb can be extended with a policy
specification indicating how the number of slaves should auto-scale. This
can be achieved manually using the 'oneenv --scaleup <envid>' or 'oneenv
--scaledown <envid>' commands. The policy specification allows the system
to automate this process subject to various conditions.

a) Time-Based Elasticity

Add the following hash to the environment specification to enforce
a time-based constraints on the number of VMs in an environment. The
'windows' parameter specifies a series of day-time restrictions and
the 'min' parameter specifies the number of VMs that should be running
(including the master VM) for the environment. The system will issue
'scaleup' or 'scaledown' requests until the number of VMs correspond to
the 'min' value.


  :elasticity_policy   => {
                           :mode => 'time-based',
                           :windows =>  {
                              'morn' => {:spec => 'Mon-Fri 0900-1300', :min => 6},
                              'aft' => {:spec => 'Mon-Fri 1300-1700', :min => 4},
                              'night' => {:spec => 'Mon-Fri 0000-0859 1700-2359', :min => 2},
                          }


b) Load-Based Elasticity

Add the following to the environment specification to support auto-scaling
based on load conditions of the VM. Currently only the cpu metric
is supported.  The 'period' parameter indicates how many periods the
condition has to be true before an action is triggered. The system
evalutes the condition every 60 seconds. The 'priority' parameter reflects
the relative priority of the environment within a service class (NOTE:
This is ignored for now).


       :elasticity_policy   => {
                                      :mode => 'auto',
                                      :min => 2,
                                      :max => 4,
                                      :priority => 10,
                                      :period => 3,
                                      :scaleup_expr   => 'avgcpu > 50',
                                      :scaledown_expr => 'avgcpu < 20'
                                      },


9. Application Storage & Stateful Applications

In order to deal with applications that must persist state independent
of the VM lifecyle, the service can make use of storage mounts from
NAS filers that are attached to the VM during the contextualization
process. Another approach is to use persistent disk images which are
attached to the VM when it is created.


The following describes how to handle NFS mounts through contextualization
using the Cassandra NoSQL database as an example. The environment
definition should have a list of mount points specified in the MOUNTLIST
variable that is passed to both the master and slave. The mounts are
expected to be in some regular form with suffix ending in a number i.e
172.30.53.23:/vol/data1, 172.30.53.23:/vol/data2, etc. The extra back
slashes are necessary to handle various shell escapes as the  mount list
variable is ultimately passed to the context script. The context script
will also recieve a SLAVE_ID (starting from 0) and NUM_SLAVES variable.


  'cassandra-flm01' => {
                .
                .
                :master_template   => "cassandra",
                :master_context_script =>  "cassandra.sh",
                :master_context_var => "\"MOUNTLIST=172.30.53.23:\\/vol\\/data{1-6},\"",
                .
                .
                :slave_template    => "cassandra",
                :slave_context_script => "cassandra.sh",
                :slave_context_var       => "\"SLAVE_ID=%SLAVE_INDEX%, NUM_SLAVES=%NUM_SLAVES%, SEEDS=%MASTER%, MOUNTLIST=172.30.53.23:\\/vol\\/data{1-6},\"",
         },



In the contextualization script the following method can be used to
extract the context variable and determine the MOUNT for a given VM in
the environment:


getmount() {
    BASE=`echo $MOUNTLIST | awk -F '{' '{print $1}'`
    if [ -z "${SLAVE_ID+x}"  ]; then
        START=`echo $MOUNTLIST | awk -F '{' '{print $2}' | awk -F '}' '{print $1}' | awk -F '-' '{START=$1;LAST=$2; print START}'`
        export MOUNT=`echo $BASE$START`
    else
         START=`echo $MOUNTLIST | awk -F '{' '{print $2}' | awk -F '}' '{print $1}' | awk -F '-' '{START=$1;LAST=$2; print START}'`
         MOUNTINDEX=`expr $START + $SLAVE_ID + 1`
         export MOUNT=`echo $BASE$MOUNTINDEX`
     fi
}


10. Environment Groups

It is possible to define a composite environment which is composed of
multiple sub-environments with dependencies in between them. The following
is a (contrived) example of an environment group that can be specified:

ENVIRONMENT = {
     .
     .
    'analytics' => {
                :type => "group",
                :description => "BI Analytics environment with Hbase data store and Tomcat cluster for processing and a Jenkins build environment",
                :members => [ 'hbase' , 'tomcat', 'apache' ],
                :dependencies => { 'tomcat' => ['hbase', 'jenkins'],
                                   'jenkins' => ['hbase'] }
        },
     .
}



-type: must be set to "group"

-members: This lists the other environments that are members of this
group. The definitions for the other environments must exist within the
environment section. Note that the order in the members list specifies
the order in which the environments are created for the group.

-dependencies: This specifices the information dependencies between
the environments. This is expressed as a string of APP_URLs that are
passed from the dependant environment to the depending environment. In
the above example, the 'hbase' environment is created first. Its
APP_URL is obtained and then passed to the 'jenkins' environments
contextualization scripts (both master and slave contextualization). The
form of the context variable is "<environment name>_URL=value" e.g
HBASE_URL="http://192.168.23.121:60010".

The context script is expected to check for that variable and setup any
configuration to point the environment to its dependency. If multiple
environments are in the dependency list, then the APP_URLS for all the
dependents will be passed as separate context variables. In the above
example, the 'tomcat' environment's contextualization should expect
variables HBASE_URL and JENKINS_URL to be set.

When the group is created, each sub-environment is assigned its own 
environment id. The first environment (the parent) id is assigned to the
group id of all subsequent environments within the group. When removing
the environments, the default is to remove each sub-environment individually.
Alternatively the entire group may be removed by using a '-' in front of the
environment id when removing i.e 'oneenv --remove -55' to remove all 
environments in the group with the parent environment id being '55'.

NOTE: When defining environments with dependencies do not use any
characters other than [a-z][a-Z][0-9]_ in the name as other characters
create problems with the context variables in OpenNebula (i.e OpenNebula
will fail to create the VM)

11. Availability Policies

The environment definition in config.rb can be extended to include
policies that relate to managing the availability of an environment.
Specifically the following use cases are considered:

- Automatically recreate VMs that are deleted (for whatever reason) -
Automatically delete and recreate VMs that go into an 'UNKNOWN' state
  typically caused by hypervisor failures
- Automatically recreate VMs in "FAILED" state caused by (hopefully)
transient
  errors in deployment
- Active-Passive environments for DR - Active-Active environments for
redundancy and load-balancing

The following gives example of two environments which form an
active-passive pair and where the active environment is set to recreate
VMs with faults.



           'tomcat_ha_active' => {
                       .
                       .
                       :availability_policy => {
                             :period => 3,
                             :recreate_expr => "nummissing > 0 || numunknown > 0 || numfailed > 0"
                             :failover_role => "active"
                             :failover_group => "tomcat-ha",
                             :failover_expr => "(numunknown + nummissing + numfail)/numvms > 0.50",
                             :failback_expr => "numunknown == 0",
                       },
           }

           'tomcat_ha_passive' => {
                       .
                       .
                        :availability_policy => {
                               :period => 3,
                               :failover_role => "passive"
                               :failover_group => "tomcat-ha",
                         },
                         :elasticity_policy   => {
                             :mode => 'time-based',
                             :windows =>  {
                                       'all'  =>   {:spec => 'Mon-Fri 0000-2359', :min => 4},
                              }},
                          :num_slaves   => 0,

                         .
           }



- period: Availability policies are evaluated every 60s. The period
is the number of times an expression has to be true before the action
(recreate, failover, or failback) is taken.

- recreate_expr: This expression is evaluated to determine if VMs should
be recreated. The variables 'nummissing', 'numunknown', 'numfailed' and
'numvms' hold the counters for the number of VMs in the environment that
are in various states.These variables can be combined in any way in a
Ruby expression.

- failover_role:  This can be one of 'active' or 'passive'

- failover_group: This identifies environments that are in the same
failover group. The name must be unique for each pair of environments
which form a failover pair.

- failover_expr: This expression is evaluated to determine if an
environment is no longer functioning and it must be failed over. The
variables 'nummissing', 'numunknown', 'numfailed' and 'numvms' hold
the counters for the number of VMs in the environment that are in
various states.These variables can be combined in any way in a Ruby
expression. When the 'failover_expr' expression is true for 'period'
intervals,the environment is marked as FAILED. The passive environment
in the same failover_group is made ACTIVE (if it isn't already)  and
elasticity policies can be used to have that environment scale out to
take over the load.

NOTE: A passive environment can be created with 0 slaves so that only
the load-balancer/master exists initially.

- failback_expr: This expression is evaluated to determine when an
environment should be considered ACTIVE again after being put into the
FAILED state. The same variables can be used as in the failover_expr. When
the 'failback_expr' is true for  'period' intervals, the environment
is marked as 'ACTIVE'. The corresponding passive environment is reset
into the PASSIVE state. Any VMs for the PASSIVE environment that were
created by elasticity polices will be shutdown.


12. Command Summary

oneenv --create <configname>

oneenv --scaleup <envid>

oneenv --scaledown <envid>

oneenv --remove <envid>

oneenv --envvms <envid>

oneenv --envs

oneenv --jobs

oneenv --vms

oneenv --upload config.rb

oneenv --upload <filename> (located in ~/vm/<filename>)

oneenv --configs

The following commands talk to the global scheduler (oneenvd-gs) process:

oneenv --services

oneenv --requests

oneenv --pools

For any informational commands the --json argument can be give to return 
the full datastructures. This can be useful for troubleshooting purposes
to display extra information that is not normally displayed.
